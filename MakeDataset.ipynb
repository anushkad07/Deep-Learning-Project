{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rgYgRJb7QaH3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgHX__8PiNqX",
        "outputId": "7d143711-4924-4570-dcfb-15b6c80ff80b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['fixed_gan.ipynb',\n",
              " 'simple_model.pth',\n",
              " '.DS_Store',\n",
              " 'models.py',\n",
              " 'measures_categories.csv',\n",
              " 'score_category.ipynb',\n",
              " 'icon_legend.json',\n",
              " 'design_topics.csv',\n",
              " 'measures_context.csv',\n",
              " '__pycache__',\n",
              " '10594-screenshot.jpg',\n",
              " 'component_legend.json',\n",
              " 'README.md',\n",
              " 'measures.csv',\n",
              " '.gitignore',\n",
              " '10594-hierarchy.json',\n",
              " 'simple_model.pt',\n",
              " 'MakeDataset.ipynb',\n",
              " 'textButton_legend.json',\n",
              " 'gflow_net.ipynb',\n",
              " 'combined',\n",
              " '.git',\n",
              " '10594-wireframe.png',\n",
              " '10594-metadata.json',\n",
              " 'model.ipynb',\n",
              " 'model_clip.ipynb',\n",
              " 'semantic_annotations']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW_KSk6RQbqa"
      },
      "source": [
        "1. Density - Anushka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qkG4XRTtQWcU"
      },
      "outputs": [],
      "source": [
        "def calculate_area(bounds):\n",
        "    return (bounds[2] - bounds[0]) * (bounds[3] - bounds[1])\n",
        "\n",
        "def do_rectangles_overlap(rect1, rect2):\n",
        "    return not (rect1[2] <= rect2[0] or rect1[0] >= rect2[2] or rect1[3] <= rect2[1] or rect1[1] >= rect2[3])\n",
        "\n",
        "def merge_overlapping_objects(data):\n",
        "    merged_objects = []\n",
        "\n",
        "    if 'children' in data:\n",
        "        for parent in data['children']:\n",
        "            non_overlapping_children = []\n",
        "\n",
        "            if 'children' in parent:\n",
        "                for child in parent['children']:\n",
        "                    overlapping = False\n",
        "                    for merged_obj in merged_objects:\n",
        "                        if do_rectangles_overlap(child['bounds'], merged_obj['bounds']):\n",
        "                            overlapping = True\n",
        "                            break\n",
        "\n",
        "                    if not overlapping:\n",
        "                        non_overlapping_children.append(child)\n",
        "\n",
        "                merged_objects.extend(non_overlapping_children)\n",
        "\n",
        "    return merged_objects\n",
        "\n",
        "def calculate_density_measure(total_area, frame_area):\n",
        "    if total_area > 0 and frame_area > 0:\n",
        "        density_measure = 1 - 2 * abs(0.5 - total_area / frame_area)\n",
        "        density_measure = max(0, min(1, density_measure))\n",
        "    else:\n",
        "        density_measure = 0\n",
        "\n",
        "    return density_measure\n",
        "\n",
        "def calculate_density(data):\n",
        "    total_area = 0\n",
        "\n",
        "    merged_objects = merge_overlapping_objects(data)\n",
        "\n",
        "    for item in merged_objects:\n",
        "        bounds = item.get('bounds')\n",
        "        if bounds:\n",
        "            area = calculate_area(bounds)\n",
        "            total_area += area\n",
        "\n",
        "    frame_bounds = data.get('bounds')\n",
        "    if frame_bounds:\n",
        "        frame_area = calculate_area(frame_bounds)\n",
        "\n",
        "    density_measure = calculate_density_measure(total_area, frame_area)\n",
        "    scaled_density_measure = 1 - (abs(density_measure - 0.5) * 2)\n",
        "\n",
        "    return scaled_density_measure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAEcvKb9QgV6"
      },
      "source": [
        "2. Colour - Yash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5lHZH4FfQfSg"
      },
      "outputs": [],
      "source": [
        "def calculate_colorfulness(image):\n",
        "    image = cv2.imread(image)\n",
        "    # Convert the image to sRGB color space\n",
        "    srgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Calculate the pixel cloud along directions (rg, yb)\n",
        "    rg = srgb_image[:,:,0] - srgb_image[:,:,1]\n",
        "    yb = (srgb_image[:,:,0] + srgb_image[:,:,1]) / 2 - srgb_image[:,:,2]\n",
        "\n",
        "    # Calculate the standard deviation and mean value along directions (rg, yb)\n",
        "    std_rg = np.std(rg)\n",
        "    std_yb = np.std(yb)\n",
        "    mean_rg = np.mean(rg)\n",
        "    mean_yb = np.mean(yb)\n",
        "\n",
        "    # Calculate ^M(3) colorfulness metric\n",
        "    colorfulness = np.sqrt(std_rg**2 + std_yb**2) + 0.3 * np.sqrt(mean_rg**2 + mean_yb**2)\n",
        "    #make colour between 0 and 1\n",
        "    # colorfulness = colorfulness / 200\n",
        "\n",
        "    return colorfulness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq6eS3APQu8n"
      },
      "source": [
        "3. Proportion - Devesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r-qyn3CleQPl"
      },
      "outputs": [],
      "source": [
        "# 1640\n",
        "# 39266\n",
        "standard_proportions = {\n",
        "        'sq': 1,        # Square\n",
        "        'r2': 1/1.414,  # Square root of 2\n",
        "        'gr': 1/1.618,  # Golden ratio\n",
        "        'r3': 1/1.732,  # Square root of 3\n",
        "        'ds': 1/2       # Double square\n",
        "    }\n",
        "\n",
        "# Recursive function to parse JSON and calculate PM_object\n",
        "def parse_json_and_calculate_PMobject(component, standard_proportions):\n",
        "    sum_min_diff = 0\n",
        "    n = 0\n",
        "\n",
        "    def extract_proportion(bounds):\n",
        "        x1, y1, x2, y2 = bounds\n",
        "        width = x2 - x1\n",
        "        height = y2 - y1\n",
        "        r = height / width if width != 0 else 0\n",
        "        if width != 0 and r <= 1:\n",
        "            return r\n",
        "        elif width != 0 and r > 1:\n",
        "            return 1/r\n",
        "        else:\n",
        "            return r\n",
        "\n",
        "    # Recursively process components and children\n",
        "    def process_component(component):\n",
        "        nonlocal sum_min_diff, n\n",
        "        bounds = component.get(\"bounds\", [])\n",
        "        if bounds:\n",
        "            proportion = extract_proportion(bounds)\n",
        "            min_diff = min(abs(proportion - sp) for sp in standard_proportions.values())\n",
        "            sum_min_diff += (1 - min_diff / 0.5)\n",
        "            n += 1\n",
        "        for child in component.get(\"children\", []):\n",
        "            process_component(child)\n",
        "\n",
        "    process_component(component)\n",
        "    return abs(sum_min_diff / n) if n != 0 else 0\n",
        "\n",
        "# Calculate the PM_object for the entire JSON data structure\n",
        "# PM_object = parse_json_and_calculate_PMobject(json_data, standard_proportions)\n",
        "# PM_object\n",
        "\n",
        "def calculate_PMlayout(width_layout, height_layout):\n",
        "    # Standard proportions as per the provided screenshot\n",
        "\n",
        "    # Calculate the layout proportion\n",
        "    r_layout = height_layout / width_layout if width_layout != 0 else 0\n",
        "    p_layout = r_layout if r_layout <= 1 else 1 / r_layout\n",
        "\n",
        "    # Find the minimum difference between p_layout and the standard proportions\n",
        "    min_diff = min(abs(p_layout - sp) for sp in standard_proportions.values())\n",
        "\n",
        "    # Calculate PM_layout according to the formula\n",
        "    PM_layout = 1 - (min_diff / 0.5)\n",
        "\n",
        "    return abs(PM_layout)\n",
        "\n",
        "# calculate_PMlayout(json_data[\"bounds\"][2],json_data[\"bounds\"][3])\n",
        "\n",
        "def calculate_PM(json_data):\n",
        "    pm_object = parse_json_and_calculate_PMobject(json_data, standard_proportions)\n",
        "    pm_layout = calculate_PMlayout(json_data[\"bounds\"][2],json_data[\"bounds\"][3])\n",
        "    return (pm_object + pm_layout) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open(\"semantic_annotations/39266.json\") as f:\n",
        "#     data = json.load(f)\n",
        "    \n",
        "# calculate_PM(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xik8zYdQwlL"
      },
      "source": [
        "4. Symmetry - Mann"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kxb4hGrTXGGG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "\n",
        "def determine_quadrant(element_center_x, element_center_y, center_x, center_y):\n",
        "    if element_center_x < center_x and element_center_y < center_y:\n",
        "        return 'UL'  # Upper-Left\n",
        "    elif element_center_x >= center_x and element_center_y < center_y:\n",
        "        return 'UR'  # Upper-Right\n",
        "    elif element_center_x < center_x and element_center_y >= center_y:\n",
        "        return 'LL'  # Lower-Left\n",
        "    elif element_center_x >= center_x and element_center_y >= center_y:\n",
        "        return 'LR'  # Lower-Right\n",
        "\n",
        "def calculate_element_properties(bounds):\n",
        "    width = bounds[2] - bounds[0]\n",
        "    height = bounds[3] - bounds[1]\n",
        "    center_x = bounds[0] + width / 2\n",
        "    center_y = bounds[1] + height / 2\n",
        "    return center_x, center_y, width, height\n",
        "\n",
        "def extract_ui_elements(data, parent_bounds=None):\n",
        "    elements = []\n",
        "    bounds = data.get('bounds', parent_bounds)\n",
        "    if 'children' not in data or not data['children']:\n",
        "        return [{'class': data['class'], 'bounds': bounds}]\n",
        "    for child in data['children']:\n",
        "        elements.extend(extract_ui_elements(child, bounds))\n",
        "    return elements\n",
        "\n",
        "# Function to calculate the symmetry score of a single UI screen\n",
        "def calculate_symmetry(data):\n",
        "    screen_bounds = data['bounds']\n",
        "    screen_center_x = (screen_bounds[2] + screen_bounds[0]) / 2\n",
        "    screen_center_y = (screen_bounds[3] + screen_bounds[1]) / 2\n",
        "\n",
        "    # Extract UI elements\n",
        "    ui_elements = extract_ui_elements(data)\n",
        "\n",
        "    # Organize elements by quadrant\n",
        "    quadrants = {'UL': [], 'UR': [], 'LL': [], 'LR': []}\n",
        "    for element in ui_elements:\n",
        "        center_x, center_y, width, height = calculate_element_properties(element['bounds'])\n",
        "        quadrant = determine_quadrant(center_x, center_y, screen_center_x, screen_center_y)\n",
        "        quadrants[quadrant].append({\n",
        "            'center_x': center_x,\n",
        "            'center_y': center_y,\n",
        "            'width': width,\n",
        "            'height': height\n",
        "        })\n",
        "\n",
        "    # Calculate averages for each quadrant\n",
        "    quadrant_sums = {key: {'x_sum': 0, 'y_sum': 0, 'width_sum': 0, 'height_sum': 0, 'count': 0}\n",
        "                     for key in quadrants.keys()}\n",
        "\n",
        "    for quadrant, elements in quadrants.items():\n",
        "        for element in elements:\n",
        "            quadrant_sums[quadrant]['x_sum'] += element['center_x']\n",
        "            quadrant_sums[quadrant]['y_sum'] += element['center_y']\n",
        "            quadrant_sums[quadrant]['width_sum'] += element['width']\n",
        "            quadrant_sums[quadrant]['height_sum'] += element['height']\n",
        "            quadrant_sums[quadrant]['count'] += 1\n",
        "\n",
        "    quadrant_averages = {}\n",
        "    for quadrant, sums in quadrant_sums.items():\n",
        "        count = sums['count']\n",
        "        if count > 0:\n",
        "            quadrant_averages[quadrant] = {\n",
        "                'avg_center_x': sums['x_sum'] / count,\n",
        "                'avg_center_y': sums['y_sum'] / count,\n",
        "                'avg_width': sums['width_sum'] / count,\n",
        "                'avg_height': sums['height_sum'] / count\n",
        "            }\n",
        "        else:\n",
        "            quadrant_averages[quadrant] = None\n",
        "\n",
        "    # Calculate symmetry values for vertical, horizontal\n",
        "    symmetry_values = {'SYM_vertical': 0, 'SYM_horizontal': 0, 'SYM_radial': 0}\n",
        "    if quadrant_averages['UL'] and quadrant_averages['UR']:\n",
        "        symmetry_values['SYM_vertical'] = 1 - (\n",
        "            abs(quadrant_averages['UL']['avg_center_x'] - quadrant_averages['UR']['avg_center_x']) /\n",
        "            screen_center_x\n",
        "        )\n",
        "\n",
        "    if quadrant_averages['UL'] and quadrant_averages['LR']:\n",
        "        symmetry_values['SYM_horizontal'] = 1 - (\n",
        "            abs(quadrant_averages['UL']['avg_center_y'] - quadrant_averages['LR']['avg_center_y']) /\n",
        "            screen_center_y\n",
        "        )\n",
        "\n",
        "    # Since we might not have elements in all quadrants, use available symmetry values\n",
        "    overall_symmetry = 0\n",
        "    symmetry_count = 0\n",
        "    for symmetry in symmetry_values.values():\n",
        "        if symmetry > 0:\n",
        "            overall_symmetry += symmetry\n",
        "            symmetry_count += 1\n",
        "\n",
        "    if symmetry_count > 0:\n",
        "        overall_symmetry /= symmetry_count  # Average symmetry score\n",
        "\n",
        "    return overall_symmetry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7PO81XaQyD6"
      },
      "source": [
        "5. Balance - Vedika"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1UW2o-nuYBIS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def screen_bounds(json_data):\n",
        "    bounds = json_data['bounds']\n",
        "    left, top, right, bottom = bounds\n",
        "    width = right - left\n",
        "    height = bottom - top\n",
        "    return width, height\n",
        "\n",
        "def parse_annotations(json_data):\n",
        "    objects = []\n",
        "    \n",
        "    def parse_children(children):\n",
        "        for item in children:\n",
        "            if 'bounds' in item:\n",
        "                left, top, right, bottom = item['bounds']\n",
        "                width = right - left\n",
        "                height = bottom - top\n",
        "                objects.append({\n",
        "                    'left': left,\n",
        "                    'top': top,\n",
        "                    'right': right,\n",
        "                    'bottom': bottom,\n",
        "                    'width': width,\n",
        "                    'height': height\n",
        "                })\n",
        "            if 'children' in item:\n",
        "                parse_children(item['children'])\n",
        "\n",
        "    parse_children(json_data['children'])\n",
        "    return objects\n",
        "\n",
        "def compute_balance_scores(objects, json_data):\n",
        "    screen_width, screen_height = screen_bounds(json_data)\n",
        "    left_area = right_area = top_area = bottom_area = 0\n",
        "    left_distance = right_distance = top_distance = bottom_distance = 0\n",
        "    \n",
        "    if objects == []:\n",
        "        return 0\n",
        "    else:\n",
        "        for obj in objects:\n",
        "            if (obj['left'] + obj['right']) / 2 < screen_width / 2:\n",
        "                left_area += obj['width'] * obj['height']\n",
        "                left_distance += abs((obj['left'] + obj['width']) / 2 - screen_width)\n",
        "            else:\n",
        "                right_area += obj['width'] * obj['height']\n",
        "                right_distance += abs((obj['left'] + obj['width']) / 2 - screen_width)\n",
        "            \n",
        "            if (obj['top'] + obj['bottom']) / 2 < screen_height / 2:\n",
        "                top_area += obj['width'] * obj['height']\n",
        "                top_distance += abs((obj['top'] + obj['height']) / 2 - screen_height)\n",
        "            else:\n",
        "                bottom_area += obj['width'] * obj['height']\n",
        "                bottom_distance += abs((obj['top'] + obj['height']) / 2 - screen_height)\n",
        "      \n",
        "        left_weight = left_area / max(left_area, right_area) if max(left_area, right_area) != 0 else 0\n",
        "        right_weight = right_area / max(left_area, right_area) if max(left_area, right_area) != 0 else 0\n",
        "        vertical_balance = abs(left_weight - right_weight)\n",
        "\n",
        "        top_weight = top_area / max(top_area, bottom_area) if max(top_area, bottom_area) != 0 else 0\n",
        "        bottom_weight = bottom_area / max(top_area, bottom_area) if max(top_area, bottom_area) != 0 else 0\n",
        "        horizontal_balance = abs(top_weight - bottom_weight)\n",
        "      \n",
        "        balance_measure = abs((vertical_balance + horizontal_balance) / 2)\n",
        "    \n",
        "    return balance_measure\n",
        "\n",
        "def balance_score(json_data):\n",
        "    objects = parse_annotations(json_data)\n",
        "    balance_score = compute_balance_scores(objects, json_data)\n",
        "    # print(\"Balance Score:\", balance_score)\n",
        "    return balance_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ7xkWwCQ5uc"
      },
      "source": [
        "**Calculate Everything**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-NfQ4Ay2h6hj"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Assuming we have a path to the zip file and a target directory\n",
        "# zip_file_path = './ui_layout_vectors.zip'  # Replace with your zip file path\n",
        "# target_directory = './'  # Replace with your target directory\n",
        "\n",
        "# # Create target directory if it does not exist\n",
        "# if not os.path.exists(target_directory):\n",
        "#     os.makedirs(target_directory)\n",
        "\n",
        "# # Extract the zip file\n",
        "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(target_directory)\n",
        "\n",
        "# # The code above assumes the paths are known and correctly provided.\n",
        "# # If you have the zip file already in your environment, you can adjust the paths accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "topics = pd.read_csv(\"design_topics.csv\")\n",
        "mapping_ids = []\n",
        "for index,i in topics.iterrows():\n",
        "    mapping_ids.append(i.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "RYAkUpvOQ74Z",
        "outputId": "e5a92cac-a532-428c-bf49-69eba22a58f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:   2%|▏         | 2010/132524 [00:26<27:34, 78.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 2000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:   3%|▎         | 4012/132524 [00:53<29:06, 73.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 4000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:   5%|▍         | 6015/132524 [01:19<28:46, 73.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 6000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:   6%|▌         | 8012/132524 [01:46<30:28, 68.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 8000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:   8%|▊         | 10011/132524 [02:13<27:34, 74.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 10000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:   9%|▉         | 12016/132524 [02:40<25:20, 79.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 12000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  11%|█         | 14016/132524 [03:07<25:50, 76.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 14000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  12%|█▏        | 16009/132524 [03:33<24:53, 78.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 16000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  14%|█▎        | 18011/132524 [04:00<27:02, 70.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 18000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  15%|█▌        | 20013/132524 [04:27<26:29, 70.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 20000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  17%|█▋        | 22008/132524 [04:54<25:40, 71.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 22000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  18%|█▊        | 24010/132524 [05:21<25:33, 70.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 24000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  20%|█▉        | 26014/132524 [05:49<23:00, 77.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 26000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  21%|██        | 28015/132524 [06:16<24:02, 72.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 28000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  23%|██▎       | 30014/132524 [06:43<23:50, 71.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 30000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  24%|██▍       | 32012/132524 [07:09<20:59, 79.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 32000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  26%|██▌       | 34013/132524 [07:37<23:43, 69.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 34000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  27%|██▋       | 36015/132524 [08:04<21:51, 73.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 36000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  29%|██▊       | 38007/132524 [08:31<24:03, 65.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 38000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  30%|███       | 40015/132524 [08:59<21:43, 71.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 40000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  32%|███▏      | 42013/132524 [09:26<18:57, 79.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 42000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  33%|███▎      | 44016/132524 [09:53<19:47, 74.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 44000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  35%|███▍      | 46014/132524 [10:21<19:04, 75.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 46000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  36%|███▌      | 48011/132524 [10:47<18:53, 74.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 48000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  38%|███▊      | 50010/132524 [11:15<20:21, 67.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 50000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  39%|███▉      | 52012/132524 [11:42<18:36, 72.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 52000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  41%|████      | 54014/132524 [12:09<18:06, 72.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 54000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  42%|████▏     | 56015/132524 [12:36<16:59, 75.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 56000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  44%|████▍     | 58008/132524 [13:03<17:19, 71.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 58000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  45%|████▌     | 60010/132524 [13:30<15:54, 75.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 60000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  47%|████▋     | 62012/132524 [13:57<15:46, 74.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 62000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  48%|████▊     | 64013/132524 [14:24<14:08, 80.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 64000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  50%|████▉     | 66009/132524 [14:51<14:12, 78.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 66000 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:  50%|████▉     | 66261/132524 [14:54<14:54, 74.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to ./measures.csv\n"
          ]
        }
      ],
      "source": [
        "output_csv = './measures.csv'\n",
        "json_folder = './semantic_annotations'\n",
        "image_folder = './combined'\n",
        "\n",
        "page_names = []\n",
        "balance_measures = []\n",
        "colour_measures = []\n",
        "symmetry_measures = []\n",
        "proportion_measures = []\n",
        "density_measures = []\n",
        "\n",
        "progress_bar = tqdm(total=len(os.listdir(json_folder)), desc=\"Processing JSON files\")\n",
        "\n",
        "for filename in os.listdir(json_folder):\n",
        "    if filename.endswith('.json'):\n",
        "        progress_bar.update(1)  # Update progress bar\n",
        "        page_number = filename.split('.')[0]\n",
        "        page_names.append(page_number)\n",
        "\n",
        "        with open(os.path.join(json_folder, filename), 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        balance = balance_score(data)\n",
        "        balance_measures.append(balance)\n",
        "\n",
        "        proportion = calculate_PM(data)\n",
        "        proportion_measures.append(proportion)\n",
        "\n",
        "        symmetry = calculate_symmetry(data)\n",
        "        symmetry_measures.append(symmetry)\n",
        "\n",
        "        density = calculate_density(data)\n",
        "        density_measures.append(density)\n",
        "\n",
        "        # Load corresponding image for colour calculation\n",
        "        image_filename = page_number + '.jpg'\n",
        "        image_path = os.path.join(image_folder, image_filename)\n",
        "        # print(image_path)\n",
        "        if os.path.exists(image_path):\n",
        "            colour = calculate_colorfulness(image_path)\n",
        "            colour_measures.append(colour)\n",
        "            # print(f\"Color value for {page_number} : {colour}\")\n",
        "        else:\n",
        "            # print(\"no file exists\")\n",
        "            colour_measures.append(None)  # Handle case where image is not found\n",
        "        # print(\"------\")\n",
        "\n",
        "        # Log progress every 1000 pages\n",
        "        if len(page_names) % 2000 == 0:\n",
        "            print(f\"Processed {len(page_names)} pages\")\n",
        "\n",
        "progress_bar.close()\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results_df = pd.DataFrame({\n",
        "    'Page': page_names,\n",
        "    'Balance': balance_measures,\n",
        "    'Colour': colour_measures,\n",
        "    'Symmetry': symmetry_measures,\n",
        "    'Proportion': proportion_measures,\n",
        "    'Density': density_measures\n",
        "})\n",
        "\n",
        "# Calculate final score (average of all measures)\n",
        "results_df['Colour'] = results_df['Colour'] / 200 # to scale between 0-1\n",
        "results_df['Final Score'] = results_df[['Balance', 'Colour', 'Symmetry', 'Proportion', 'Density']].mean(axis=1)\n",
        "# results_df['Final Score'] = results_df[['Balance', 'Symmetry', 'Proportion', 'Density']].mean(axis=1)\n",
        "\n",
        "# Filtering outliers (dropping 1 out of 300 entries, negligible)\n",
        "results_df = results_df[results_df['Final Score'] <= 1] \n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(\"Results saved to\", output_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing JSON files:   1%|          | 1460/132524 [00:17<26:48, 81.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to ./measures_context.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "output_csv = './measures_context.csv'\n",
        "json_folder = './semantic_annotations'\n",
        "image_folder = './combined'\n",
        "\n",
        "page_names = []\n",
        "balance_measures = []\n",
        "colour_measures = []\n",
        "symmetry_measures = []\n",
        "proportion_measures = []\n",
        "density_measures = []\n",
        "descriptions = []\n",
        "\n",
        "progress_bar = tqdm(total=len(os.listdir(json_folder)), desc=\"Processing JSON files\")\n",
        "\n",
        "for c,i in enumerate(mapping_ids):\n",
        "    filename = f\"{i[0]}.json\"\n",
        "    if filename.endswith('.json'):\n",
        "        progress_bar.update(1)  # Update progress bar\n",
        "\n",
        "        if os.path.isfile(os.path.join(json_folder, filename)):\n",
        "            with open(os.path.join(json_folder, filename), 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "        else:\n",
        "            continue\n",
        "        \n",
        "        page_number = filename.split('.')[0]\n",
        "        page_names.append(page_number)\n",
        "\n",
        "        balance = balance_score(data)\n",
        "        balance_measures.append(balance)\n",
        "\n",
        "        proportion = calculate_PM(data)\n",
        "        proportion_measures.append(proportion)\n",
        "\n",
        "        symmetry = calculate_symmetry(data)\n",
        "        symmetry_measures.append(symmetry)\n",
        "\n",
        "        density = calculate_density(data)\n",
        "        density_measures.append(density)\n",
        "        \n",
        "        descriptions.append(i[1])\n",
        "\n",
        "        # Load corresponding image for colour calculation\n",
        "        image_filename = page_number + '.jpg'\n",
        "        image_path = os.path.join(image_folder, image_filename)\n",
        "        # print(image_path)\n",
        "        if os.path.exists(image_path):\n",
        "            colour = calculate_colorfulness(image_path)\n",
        "            colour_measures.append(colour)\n",
        "            # print(f\"Color value for {page_number} : {colour}\")\n",
        "        else:\n",
        "            # print(\"no file exists\")\n",
        "            colour_measures.append(None)  # Handle case where image is not found\n",
        "        # print(\"------\")\n",
        "\n",
        "        # Log progress every 1000 pages\n",
        "        if len(page_names) % 2000 == 0:\n",
        "            print(f\"Processed {len(page_names)} pages\")\n",
        "\n",
        "progress_bar.close()\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results_df = pd.DataFrame({\n",
        "    'Page': page_names,\n",
        "    'Balance': balance_measures,\n",
        "    'Colour': colour_measures,\n",
        "    'Symmetry': symmetry_measures,\n",
        "    'Proportion': proportion_measures,\n",
        "    'Density': density_measures,\n",
        "    'Description': descriptions\n",
        "})\n",
        "\n",
        "# Calculate final score (average of all measures)\n",
        "results_df['Colour'] = results_df['Colour'] / 200 # to scale between 0-1\n",
        "results_df['Final Score'] = results_df[['Balance', 'Colour', 'Symmetry', 'Proportion', 'Density']].mean(axis=1)\n",
        "# results_df['Final Score'] = results_df[['Balance', 'Symmetry', 'Proportion', 'Density']].mean(axis=1)\n",
        "\n",
        "# Filtering outliers (dropping 1 out of 300 entries, negligible)\n",
        "results_df = results_df[results_df['Final Score'] <= 1] \n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(\"Results saved to\", output_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
